# 对于相关概念的理解
## 有监督学习
算法的结果是明确的，比如人脸识别
## 无监督学习
算法的结果不明确，没有对与错之分，如聚类
## 泛化能力
泛化能力是对新样本的适应能力，个人理解也就是新的样本是否能够正确算出结果，泛化能力也可以看做对算法适用范围的一种标识
## 过拟合和欠拟合
过拟合和欠拟合相对，过拟合指太过于缩小训练数据集与模型的差异，导致无法适用测试数据；欠拟合正好相反，指模型过于简单，不能很好表现出数据的规律。
## 方差和偏差以及各自解决办法
方差指实际值于数据期望的偏离程度，统计学上是随机变量与其均值的偏离程度，即*E{[X-E(X)]^2}*,用于表征稳定性
偏差指模型输出结果的期望值与样本真实情况的差距
## 交叉验证
交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。
**较差验证主要有3种形式**
### Holdout 验证
常识来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。
### K-fold cross-validation
K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的 [3]  。
### 留一验证
正如名称所建议， 留一验证（LOOCV）意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于和K-fold 交叉验证是一样的，其中K为原本样本个数。 在某些情况下是存在有效率的演算法，如使用kernel regression 和Tikhonov regularization。
## 线性回归的原理
线性回归即常见的*y=ax+b*形式，线性回归的预测结果是连续的值
## 线性回归损失函数、代价函数、目标函数
损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差，用来估量模型预测值 f(x)与真实值 Y的不一致程度，通常用 L(Y,f(x))来表示。
代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。
目标函数（Object Function）定义为：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。
## 优化方法(梯度下降法、牛顿法、拟牛顿法
[https://www.cnblogs.com/maybe2030/p/4751804.html#_label0]  
## 线性回归的评估指标
MSE均方误差
RMSE均方根误差
MAE平均绝对
R方
[https://www.jianshu.com/p/9ee85fdad150]
## sklearn参数详解
调用sklearn.linear_model.LinearRegression()所需参数： 
* fit_intercept : 布尔型参数，表示是否计算该模型截距。可选参数。 
* normalize : 布尔型参数，若为True，则X在回归前进行归一化。可选参数。默认值为False。 
* copy_X : 布尔型参数，若为True，则X将被复制；否则将被覆盖。 可选参数。默认值为True。 
* n_jobs : 整型参数，表示用于计算的作业数量；若为-1，则用所有的CPU。可选参数。默认值为1。

线性回归fit函数用于拟合输入输出数据，调用形式为model.fit(X,y, sample_weight=None)： 
• X : X为训练向量； 
• y : y为相对于X的目标向量； 
• sample_weight : 分配给各个样本的权重数组，一般不需要使用，可省略。 
注意：X，y 以及model.fit()返回的值都是2-D数组，如：a= [ [ 0] ]
[https://blog.csdn.net/hubingshabi/article/details/80172608 ]
